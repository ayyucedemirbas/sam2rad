{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1VzB26SlIfTdEUJRfdozJLFX2FiwppkQ4",
      "authorship_tag": "ABX9TyNEiIR8qS1jKgN8zkG2Fq2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayyucedemirbas/sam2rad/blob/main/SamRadiology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aswahd/SamRadiology.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zJpWR_v1h6Y",
        "outputId": "cd9a0380-04bd-41e5-c4cf-f21fe26f679f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SamRadiology'...\n",
            "remote: Enumerating objects: 239, done.\u001b[K\n",
            "remote: Counting objects: 100% (239/239), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 239 (delta 101), reused 211 (delta 75), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (239/239), 317.65 KiB | 8.36 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd SamRadiology"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tV_SD3o51p5v",
        "outputId": "9736a885-05c1-4077-b4cb-7a86611a5f5e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SamRadiology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "LMSfFrPT1tTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJsdVvRd2Ygw",
        "outputId": "b3325f6c-cb13-4f06-8c9f-6569bd52244b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SamRadiology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir weights"
      ],
      "metadata": {
        "id": "ySKK9IF63y2Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " %cd weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9QX5PYKb5nu",
        "outputId": "0aaec7db-950f-40c8-9a37-d472d3ba91f1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SamRadiology/weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pvZ8hls09t6",
        "outputId": "a2c3ba6e-25da-456f-fcaa-58dc460c38d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-12 17:41:46--  https://huggingface.co/facebook/sam2-hiera-tiny/resolve/f245b47be73d8858fb7543a8b9c1c720d9f98779/sam2_hiera_tiny.pt\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.40, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/72/89/72894457a00355999d730b0c58197c0b0ca55cc8061e43ed620038cff9cb4298/65b50056e05bcb13694174f51bb6da89c894b57b75ccdf0ba6352c597c5d1125?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sam2_hiera_tiny.pt%3B+filename%3D%22sam2_hiera_tiny.pt%22%3B&Expires=1741804906&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTgwNDkwNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzcyLzg5LzcyODk0NDU3YTAwMzU1OTk5ZDczMGIwYzU4MTk3YzBiMGNhNTVjYzgwNjFlNDNlZDYyMDAzOGNmZjljYjQyOTgvNjViNTAwNTZlMDViY2IxMzY5NDE3NGY1MWJiNmRhODljODk0YjU3Yjc1Y2NkZjBiYTYzNTJjNTk3YzVkMTEyNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lAeGF8BacKoT1HxdHhY%7E9ByTtB0THks6GWotxFdiZXbrD2czCWj4MlICKF6yYyWVlPyclUBHt0tqkzlwQlglLyCYztSXmrPAWBzQ39mWqilwNCHV8Qxgp65FQkyFMIRN3vgOocwY5xKqNIOFYzpnjzQ7Bcnd6r-kPxazCNGHtds8OFY81znzBjt2TeIZLObC1FOlCfGxTqaKHnatyc5jAVMz7gqqVCFC0aCvjIYSAl-oAPhM2fN54rp0XzyMpLoppdwwhQXh1NoTOBw0DpTH0mlfi8TCLkIeNOO28D7sHG2UMrZSC3QAeH8Rj55xckG27fUYeW0NGW7rLLCtWsqWEg__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-12 17:41:46--  https://cdn-lfs-us-1.hf.co/repos/72/89/72894457a00355999d730b0c58197c0b0ca55cc8061e43ed620038cff9cb4298/65b50056e05bcb13694174f51bb6da89c894b57b75ccdf0ba6352c597c5d1125?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27sam2_hiera_tiny.pt%3B+filename%3D%22sam2_hiera_tiny.pt%22%3B&Expires=1741804906&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTgwNDkwNn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzcyLzg5LzcyODk0NDU3YTAwMzU1OTk5ZDczMGIwYzU4MTk3YzBiMGNhNTVjYzgwNjFlNDNlZDYyMDAzOGNmZjljYjQyOTgvNjViNTAwNTZlMDViY2IxMzY5NDE3NGY1MWJiNmRhODljODk0YjU3Yjc1Y2NkZjBiYTYzNTJjNTk3YzVkMTEyNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=lAeGF8BacKoT1HxdHhY%7E9ByTtB0THks6GWotxFdiZXbrD2czCWj4MlICKF6yYyWVlPyclUBHt0tqkzlwQlglLyCYztSXmrPAWBzQ39mWqilwNCHV8Qxgp65FQkyFMIRN3vgOocwY5xKqNIOFYzpnjzQ7Bcnd6r-kPxazCNGHtds8OFY81znzBjt2TeIZLObC1FOlCfGxTqaKHnatyc5jAVMz7gqqVCFC0aCvjIYSAl-oAPhM2fN54rp0XzyMpLoppdwwhQXh1NoTOBw0DpTH0mlfi8TCLkIeNOO28D7sHG2UMrZSC3QAeH8Rj55xckG27fUYeW0NGW7rLLCtWsqWEg__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.75.19, 3.165.75.33, 3.165.75.112, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.75.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155906050 (149M) [binary/octet-stream]\n",
            "Saving to: ‘sam2_hiera_tiny.pt’\n",
            "\n",
            "sam2_hiera_tiny.pt  100%[===================>] 148.68M  23.4MB/s    in 6.4s    \n",
            "\n",
            "2025-03-12 17:41:53 (23.3 MB/s) - ‘sam2_hiera_tiny.pt’ saved [155906050/155906050]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/facebook/sam2-hiera-tiny/resolve/f245b47be73d8858fb7543a8b9c1c720d9f98779/sam2_hiera_tiny.pt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_F9WmpceSak",
        "outputId": "1dfa25fa-d930-4090-cf2c-3500aad8d264"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/SamRadiology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --config /content/SamRadiology/config.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo6-Q8n85K3D",
        "outputId": "38f76768-16e9-4a63-d483-1799eeebd418"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/SamRadiology', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/tmp/tmpmp2v4o_w', '/content/SamRadiology/sam2rad/models']\n",
            "Found 99 training images and 99 masks.\n",
            "Found 1793 validation images and 1793 masks.\n",
            "I20250312 17:49:45 4136 sam2rad train.py:415] Train dataset size: 99\n",
            "I20250312 17:49:45 4136 sam2rad train.py:415] Train dataset size: 99\n",
            "I20250312 17:49:45 4136 sam2rad train.py:416] Validation dataset size: 100\n",
            "I20250312 17:49:45 4136 sam2rad train.py:416] Validation dataset size: 100\n",
            "I20250312 17:49:46 4136 sam2rad build_encoder.py:608] ImageEncoder loaded checkpoint from /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_encoder.py:608] ImageEncoder loaded checkpoint from /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:232] MemoryAttention loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:232] MemoryAttention loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:277] MemoryEncoder loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:277] MemoryEncoder loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:309] PromptEncoder loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:309] PromptEncoder loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:327] Prompt sampler loaded successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_model.py:327] Prompt sampler loaded successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_decoder.py:457] MaskDecoder loaded checkpoint from /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:46 4136 sam2rad build_decoder.py:457] MaskDecoder loaded checkpoint from /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:47 4136 sam2rad build_model.py:368] MLP and Linear loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:47 4136 sam2rad build_model.py:368] MLP and Linear loaded from checkpoint /content/SamRadiology/weights/sam2_hiera_tiny.pt successfully.\n",
            "I20250312 17:49:47 4136 sam2rad train.py:437] SegmentationModule(\n",
            "  (model): Model(\n",
            "    (image_encoder): Sam2TinyHieraAdapter(\n",
            "      (net): ImageEncoder(\n",
            "        (trunk): Hiera(\n",
            "          (patch_embed): PatchEmbed(\n",
            "            (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (1): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=96, out_features=576, bias=True)\n",
            "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=96, out_features=192, bias=True)\n",
            "            )\n",
            "            (2): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (3): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=192, out_features=1152, bias=True)\n",
            "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=192, out_features=384, bias=True)\n",
            "            )\n",
            "            (4-9): 6 x MultiScaleBlock(\n",
            "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (10): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=384, out_features=2304, bias=True)\n",
            "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=384, out_features=768, bias=True)\n",
            "            )\n",
            "            (11): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (neck): FpnNeck(\n",
            "          (position_encoding): PositionEmbeddingSine()\n",
            "          (convs): ModuleList(\n",
            "            (0): Sequential(\n",
            "              (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (3): Sequential(\n",
            "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (adapters): ModuleList(\n",
            "        (0-1): 2 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=96, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (2-3): 2 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=192, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (4-10): 7 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=384, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (11): AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=768, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (memory_attention): MemoryAttention(\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x MemoryAttentionLayer(\n",
            "          (self_attn): RoPEAttention(\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (cross_attn_image): RoPEAttention(\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (memory_encoder): MemoryEncoder(\n",
            "      (mask_downsampler): MaskDownSampler(\n",
            "        (encoder): Sequential(\n",
            "          (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (4): LayerNorm2d()\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (7): LayerNorm2d()\n",
            "          (8): GELU(approximate='none')\n",
            "          (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (10): LayerNorm2d()\n",
            "          (11): GELU(approximate='none')\n",
            "          (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fuser): Fuser(\n",
            "        (proj): Identity()\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x CXBlock(\n",
            "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "            (norm): LayerNorm2d()\n",
            "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop_path): Identity()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (position_encoding): PositionEmbeddingSine()\n",
            "      (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (prompt_sampler): PromptSampler(\n",
            "      (prompt_learner): Sam2HighResPPN(\n",
            "        (pos_encoding2d): PositionEmbeddingSine()\n",
            "        (pos_encoding1d): PositionEmbeddingSine1D()\n",
            "        (box_regression_head): BoxRegressionHead(\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "              (1): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "            (act): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (mask_classifier): MaskClassifier(\n",
            "          (layers): ModuleList(\n",
            "            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "          (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (prompt_encoder): PromptEncoder(\n",
            "          (pe_layer): PositionEmbeddingRandom()\n",
            "          (point_embeddings): ModuleList(\n",
            "            (0-3): 4 x Embedding(1, 256)\n",
            "          )\n",
            "          (not_a_point_embed): Embedding(1, 256)\n",
            "          (mask_downscaling): Sequential(\n",
            "            (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "            (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "            (4): LayerNorm2d()\n",
            "            (5): GELU(approximate='none')\n",
            "            (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (no_mask_embed): Embedding(1, 256)\n",
            "        )\n",
            "        (channel_adapters): ModuleList(\n",
            "          (0): Identity()\n",
            "          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x TwoWayAttentionBlock(\n",
            "            (self_attn): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_token_to_image): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "            )\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): MLP(\n",
            "              (layers): ModuleList(\n",
            "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "                (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "              )\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_image_to_token): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (prompt_encoder): PromptEncoder(\n",
            "        (pe_layer): PositionEmbeddingRandom()\n",
            "        (point_embeddings): ModuleList(\n",
            "          (0-3): 4 x Embedding(1, 256)\n",
            "        )\n",
            "        (not_a_point_embed): Embedding(1, 256)\n",
            "        (mask_downscaling): Sequential(\n",
            "          (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (4): LayerNorm2d()\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (no_mask_embed): Embedding(1, 256)\n",
            "      )\n",
            "    )\n",
            "    (sam_mask_decoder): SAM2LoRAMaskDecoder(\n",
            "      (net): MaskDecoder(\n",
            "        (transformer): TwoWayTransformer(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x TwoWayAttentionBlock(\n",
            "              (self_attn): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=256, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=256, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (cross_attn_token_to_image): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "              )\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "                  (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "                )\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (cross_attn_image_to_token): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (final_attn_token_to_image): Attention(\n",
            "            (q_proj): LoRAqkv(\n",
            "              (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "              (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "            )\n",
            "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (v_proj): LoRAqkv(\n",
            "              (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "              (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "            )\n",
            "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (iou_token): Embedding(1, 256)\n",
            "        (mask_tokens): Embedding(4, 256)\n",
            "        (obj_score_token): Embedding(1, 256)\n",
            "        (output_upscaling): Sequential(\n",
            "          (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (4): GELU(approximate='none')\n",
            "        )\n",
            "        (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (output_hypernetworks_mlps): ModuleList(\n",
            "          (0-3): 4 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=32, bias=True)\n",
            "            )\n",
            "            (act): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (iou_prediction_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "          (act): ReLU()\n",
            "        )\n",
            "        (pred_obj_score_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=1, bias=True)\n",
            "          )\n",
            "          (act): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (obj_ptr_proj): MLP(\n",
            "      (layers): ModuleList(\n",
            "        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "      (act): ReLU()\n",
            "    )\n",
            "    (obj_ptr_tpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (learnable_prompts): ParameterDict(  (acdc): Parameter containing: [torch.FloatTensor of size 3x10x256])\n",
            ")\n",
            "I20250312 17:49:47 4136 sam2rad train.py:437] SegmentationModule(\n",
            "  (model): Model(\n",
            "    (image_encoder): Sam2TinyHieraAdapter(\n",
            "      (net): ImageEncoder(\n",
            "        (trunk): Hiera(\n",
            "          (patch_embed): PatchEmbed(\n",
            "            (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
            "          )\n",
            "          (blocks): ModuleList(\n",
            "            (0): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
            "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=96, out_features=384, bias=True)\n",
            "                  (1): Linear(in_features=384, out_features=96, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (1): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=96, out_features=576, bias=True)\n",
            "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=96, out_features=192, bias=True)\n",
            "            )\n",
            "            (2): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
            "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=192, out_features=768, bias=True)\n",
            "                  (1): Linear(in_features=768, out_features=192, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (3): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=192, out_features=1152, bias=True)\n",
            "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=192, out_features=384, bias=True)\n",
            "            )\n",
            "            (4-9): 6 x MultiScaleBlock(\n",
            "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
            "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
            "                  (1): Linear(in_features=1536, out_features=384, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "            (10): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
            "              (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "                (qkv): Linear(in_features=384, out_features=2304, bias=True)\n",
            "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "              (proj): Linear(in_features=384, out_features=768, bias=True)\n",
            "            )\n",
            "            (11): MultiScaleBlock(\n",
            "              (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (attn): MultiScaleAttention(\n",
            "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
            "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              )\n",
            "              (drop_path): Identity()\n",
            "              (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                  (1): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                )\n",
            "                (act): GELU(approximate='none')\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (neck): FpnNeck(\n",
            "          (position_encoding): PositionEmbeddingSine()\n",
            "          (convs): ModuleList(\n",
            "            (0): Sequential(\n",
            "              (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (1): Sequential(\n",
            "              (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (2): Sequential(\n",
            "              (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "            (3): Sequential(\n",
            "              (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (adapters): ModuleList(\n",
            "        (0-1): 2 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=96, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=96, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (2-3): 2 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=192, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (4-10): 7 x AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=384, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=384, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "        (11): AdapterBlock(\n",
            "          (adapters): Sequential(\n",
            "            (0): Linear(in_features=768, out_features=64, bias=True)\n",
            "            (1): GELU(approximate='none')\n",
            "            (2): Linear(in_features=64, out_features=768, bias=True)\n",
            "            (3): GELU(approximate='none')\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (memory_attention): MemoryAttention(\n",
            "      (layers): ModuleList(\n",
            "        (0-3): 4 x MemoryAttentionLayer(\n",
            "          (self_attn): RoPEAttention(\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (cross_attn_image): RoPEAttention(\n",
            "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (memory_encoder): MemoryEncoder(\n",
            "      (mask_downsampler): MaskDownSampler(\n",
            "        (encoder): Sequential(\n",
            "          (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (4): LayerNorm2d()\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (7): LayerNorm2d()\n",
            "          (8): GELU(approximate='none')\n",
            "          (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (10): LayerNorm2d()\n",
            "          (11): GELU(approximate='none')\n",
            "          (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "      )\n",
            "      (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (fuser): Fuser(\n",
            "        (proj): Identity()\n",
            "        (layers): ModuleList(\n",
            "          (0-1): 2 x CXBlock(\n",
            "            (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
            "            (norm): LayerNorm2d()\n",
            "            (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop_path): Identity()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (position_encoding): PositionEmbeddingSine()\n",
            "      (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (prompt_sampler): PromptSampler(\n",
            "      (prompt_learner): Sam2HighResPPN(\n",
            "        (pos_encoding2d): PositionEmbeddingSine()\n",
            "        (pos_encoding1d): PositionEmbeddingSine1D()\n",
            "        (box_regression_head): BoxRegressionHead(\n",
            "          (mlp): MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "              (1): Linear(in_features=256, out_features=4, bias=True)\n",
            "            )\n",
            "            (act): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (mask_classifier): MaskClassifier(\n",
            "          (layers): ModuleList(\n",
            "            (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "            (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "          )\n",
            "          (dropout): Dropout2d(p=0.1, inplace=False)\n",
            "          (final_conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (prompt_encoder): PromptEncoder(\n",
            "          (pe_layer): PositionEmbeddingRandom()\n",
            "          (point_embeddings): ModuleList(\n",
            "            (0-3): 4 x Embedding(1, 256)\n",
            "          )\n",
            "          (not_a_point_embed): Embedding(1, 256)\n",
            "          (mask_downscaling): Sequential(\n",
            "            (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
            "            (1): LayerNorm2d()\n",
            "            (2): GELU(approximate='none')\n",
            "            (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "            (4): LayerNorm2d()\n",
            "            (5): GELU(approximate='none')\n",
            "            (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          )\n",
            "          (no_mask_embed): Embedding(1, 256)\n",
            "        )\n",
            "        (channel_adapters): ModuleList(\n",
            "          (0): Identity()\n",
            "          (1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (2): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x TwoWayAttentionBlock(\n",
            "            (self_attn): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "            )\n",
            "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_token_to_image): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "            )\n",
            "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): MLP(\n",
            "              (layers): ModuleList(\n",
            "                (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "                (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "              )\n",
            "              (act): ReLU()\n",
            "            )\n",
            "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "            (cross_attn_image_to_token): Attention(\n",
            "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (prompt_encoder): PromptEncoder(\n",
            "        (pe_layer): PositionEmbeddingRandom()\n",
            "        (point_embeddings): ModuleList(\n",
            "          (0-3): 4 x Embedding(1, 256)\n",
            "        )\n",
            "        (not_a_point_embed): Embedding(1, 256)\n",
            "        (mask_downscaling): Sequential(\n",
            "          (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (4): LayerNorm2d()\n",
            "          (5): GELU(approximate='none')\n",
            "          (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
            "        )\n",
            "        (no_mask_embed): Embedding(1, 256)\n",
            "      )\n",
            "    )\n",
            "    (sam_mask_decoder): SAM2LoRAMaskDecoder(\n",
            "      (net): MaskDecoder(\n",
            "        (transformer): TwoWayTransformer(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x TwoWayAttentionBlock(\n",
            "              (self_attn): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=256, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=256, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "              )\n",
            "              (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (cross_attn_token_to_image): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "              )\n",
            "              (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (mlp): MLP(\n",
            "                (layers): ModuleList(\n",
            "                  (0): Linear(in_features=256, out_features=2048, bias=True)\n",
            "                  (1): Linear(in_features=2048, out_features=256, bias=True)\n",
            "                )\n",
            "                (act): ReLU()\n",
            "              )\n",
            "              (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "              (cross_attn_image_to_token): Attention(\n",
            "                (q_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                (v_proj): LoRAqkv(\n",
            "                  (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "                  (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "                  (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "                )\n",
            "                (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (final_attn_token_to_image): Attention(\n",
            "            (q_proj): LoRAqkv(\n",
            "              (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "              (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "            )\n",
            "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "            (v_proj): LoRAqkv(\n",
            "              (proj): Linear(in_features=256, out_features=128, bias=True)\n",
            "              (w_a): Linear(in_features=256, out_features=8, bias=False)\n",
            "              (w_b): Linear(in_features=8, out_features=128, bias=False)\n",
            "            )\n",
            "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
            "          )\n",
            "          (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (iou_token): Embedding(1, 256)\n",
            "        (mask_tokens): Embedding(4, 256)\n",
            "        (obj_score_token): Embedding(1, 256)\n",
            "        (output_upscaling): Sequential(\n",
            "          (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (1): LayerNorm2d()\n",
            "          (2): GELU(approximate='none')\n",
            "          (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "          (4): GELU(approximate='none')\n",
            "        )\n",
            "        (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "        (output_hypernetworks_mlps): ModuleList(\n",
            "          (0-3): 4 x MLP(\n",
            "            (layers): ModuleList(\n",
            "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "              (2): Linear(in_features=256, out_features=32, bias=True)\n",
            "            )\n",
            "            (act): ReLU()\n",
            "          )\n",
            "        )\n",
            "        (iou_prediction_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=4, bias=True)\n",
            "          )\n",
            "          (act): ReLU()\n",
            "        )\n",
            "        (pred_obj_score_head): MLP(\n",
            "          (layers): ModuleList(\n",
            "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
            "            (2): Linear(in_features=256, out_features=1, bias=True)\n",
            "          )\n",
            "          (act): ReLU()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (obj_ptr_proj): MLP(\n",
            "      (layers): ModuleList(\n",
            "        (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
            "      )\n",
            "      (act): ReLU()\n",
            "    )\n",
            "    (obj_ptr_tpos_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (learnable_prompts): ParameterDict(  (acdc): Parameter containing: [torch.FloatTensor of size 3x10x256])\n",
            ")\n",
            "\u001b[31mmodel.image_encoder.adapters.0.adapters.0.weight | torch.Size([64, 96])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.0.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.0.adapters.2.weight | torch.Size([96, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.0.adapters.2.bias | torch.Size([96])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.1.adapters.0.weight | torch.Size([64, 96])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.1.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.1.adapters.2.weight | torch.Size([96, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.1.adapters.2.bias | torch.Size([96])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.2.adapters.0.weight | torch.Size([64, 192])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.2.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.2.adapters.2.weight | torch.Size([192, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.2.adapters.2.bias | torch.Size([192])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.3.adapters.0.weight | torch.Size([64, 192])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.3.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.3.adapters.2.weight | torch.Size([192, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.3.adapters.2.bias | torch.Size([192])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.4.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.4.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.4.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.4.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.5.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.5.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.5.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.5.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.6.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.6.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.6.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.6.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.7.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.7.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.7.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.7.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.8.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.8.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.8.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.8.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.9.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.9.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.9.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.9.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.10.adapters.0.weight | torch.Size([64, 384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.10.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.10.adapters.2.weight | torch.Size([384, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.10.adapters.2.bias | torch.Size([384])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.11.adapters.0.weight | torch.Size([64, 768])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.11.adapters.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.11.adapters.2.weight | torch.Size([768, 64])\u001b[0m\n",
            "\u001b[31mmodel.image_encoder.adapters.11.adapters.2.bias | torch.Size([768])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.box_regression_head.mlp.layers.0.weight | torch.Size([256, 512])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.box_regression_head.mlp.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.box_regression_head.mlp.layers.1.weight | torch.Size([4, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.box_regression_head.mlp.layers.1.bias | torch.Size([4])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.layers.0.weight | torch.Size([64, 256, 3, 3])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.layers.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.layers.1.weight | torch.Size([64, 64, 3, 3])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.layers.1.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.final_conv.weight | torch.Size([1, 64, 1, 1])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.mask_classifier.final_conv.bias | torch.Size([1])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.channel_adapters.1.weight | torch.Size([256, 64, 1, 1])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.channel_adapters.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.channel_adapters.2.weight | torch.Size([256, 32, 1, 1])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.channel_adapters.2.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.q_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.q_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.k_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.k_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.v_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.v_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.out_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.self_attn.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm1.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_token_to_image.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm2.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm2.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.mlp.layers.0.weight | torch.Size([2048, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.mlp.layers.0.bias | torch.Size([2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.mlp.layers.1.weight | torch.Size([256, 2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.mlp.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm3.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm3.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm4.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.norm4.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.0.cross_attn_image_to_token.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.q_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.q_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.k_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.k_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.v_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.v_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.out_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.self_attn.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm1.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_token_to_image.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm2.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm2.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.mlp.layers.0.weight | torch.Size([2048, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.mlp.layers.0.bias | torch.Size([2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.mlp.layers.1.weight | torch.Size([256, 2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.mlp.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm3.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm3.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm4.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.norm4.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.1.cross_attn_image_to_token.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.q_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.q_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.k_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.k_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.v_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.v_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.out_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.self_attn.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm1.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_token_to_image.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm2.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm2.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.mlp.layers.0.weight | torch.Size([2048, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.mlp.layers.0.bias | torch.Size([2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.mlp.layers.1.weight | torch.Size([256, 2048])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.mlp.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm3.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm3.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm4.weight | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.norm4.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.q_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.q_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.k_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.k_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.v_proj.weight | torch.Size([128, 256])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.v_proj.bias | torch.Size([128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.out_proj.weight | torch.Size([256, 128])\u001b[0m\n",
            "\u001b[31mmodel.prompt_sampler.prompt_learner.layers.2.cross_attn_image_to_token.out_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.self_attn.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.self_attn.q_proj.w_b.weight | torch.Size([256, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.self_attn.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.self_attn.v_proj.w_b.weight | torch.Size([256, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_token_to_image.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_token_to_image.q_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_token_to_image.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_token_to_image.v_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_image_to_token.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_image_to_token.q_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_image_to_token.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.0.cross_attn_image_to_token.v_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.self_attn.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.self_attn.q_proj.w_b.weight | torch.Size([256, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.self_attn.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.self_attn.v_proj.w_b.weight | torch.Size([256, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_token_to_image.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_token_to_image.q_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_token_to_image.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_token_to_image.v_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_image_to_token.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_image_to_token.q_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_image_to_token.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.layers.1.cross_attn_image_to_token.v_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.final_attn_token_to_image.q_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.final_attn_token_to_image.q_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.final_attn_token_to_image.v_proj.w_a.weight | torch.Size([8, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.transformer.final_attn_token_to_image.v_proj.w_b.weight | torch.Size([128, 8])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.mask_tokens.weight | torch.Size([4, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.obj_score_token.weight | torch.Size([1, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.0.weight | torch.Size([256, 64, 2, 2])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.0.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.1.weight | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.1.bias | torch.Size([64])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.3.weight | torch.Size([64, 32, 2, 2])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_upscaling.3.bias | torch.Size([32])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.2.weight | torch.Size([32, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.0.layers.2.bias | torch.Size([32])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.2.weight | torch.Size([32, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.1.layers.2.bias | torch.Size([32])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.2.weight | torch.Size([32, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.2.layers.2.bias | torch.Size([32])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.2.weight | torch.Size([32, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.output_hypernetworks_mlps.3.layers.2.bias | torch.Size([32])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.2.weight | torch.Size([1, 256])\u001b[0m\n",
            "\u001b[31mmodel.sam_mask_decoder.net.pred_obj_score_head.layers.2.bias | torch.Size([1])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.0.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.0.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.1.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.1.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.2.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_proj.layers.2.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_tpos_proj.weight | torch.Size([256, 256])\u001b[0m\n",
            "\u001b[31mmodel.obj_ptr_tpos_proj.bias | torch.Size([256])\u001b[0m\n",
            "\u001b[31mlearnable_prompts.acdc | torch.Size([3, 10, 256])\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 8if37lck.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                   | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | model | SegmentationModule     | 44.7 M | train\n",
            "1 | iou   | MulticlassJaccardIndex | 0      | train\n",
            "2 | dice  | Dice                   | 0      | train\n",
            "---------------------------------------------------------\n",
            "6.7 M     Trainable params\n",
            "38.0 M    Non-trainable params\n",
            "44.7 M    Total params\n",
            "178.672   Total estimated model params size (MB)\n",
            "635       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Epoch 2:  41% 20/49 [00:40<00:58,  2.01s/it, v_num=7lck, train_loss_seg=0.487, interim_mask_loss=0.0548, train_loss_box=0.863, train_loss_object=0.362, train_iou=0.121, train_dice=0.151]"
          ]
        }
      ]
    }
  ]
}